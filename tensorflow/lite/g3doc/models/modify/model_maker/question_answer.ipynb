{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2q27gKz1H20"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUfAcER1oUS6"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb7qyhNL1yWt"
      },
      "source": [
        "# BERT Question Answer with TensorFlow Lite Model Maker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fw5Y7snSuG51"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/lite/models/modify/model_maker/question_answer\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/question_answer.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/modify/model_maker/question_answer.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/tensorflow/tensorflow/lite/g3doc/models/modify/model_maker/question_answer.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sr3q-gvm3cI8"
      },
      "source": [
        "The [TensorFlow Lite Model Maker library](https://www.tensorflow.org/lite/models/modify/model_maker) simplifies the process of adapting and converting a TensorFlow model to particular input data when deploying this model for on-device ML applications.\n",
        "\n",
        "This notebook shows an end-to-end example that utilizes the Model Maker library to illustrate the adaptation and conversion of a commonly-used question answer model for question answer task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxEHFTk755qw"
      },
      "source": [
        "# Introduction to BERT Question Answer Task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFbKTCF25-SG"
      },
      "source": [
        "The supported task in this library is extractive question answer task, which means given a passage and a question, the answer is the span in the passage. The image below shows an example for question answer.\n",
        "\n",
        "\n",
        "<p align=\"center\"><img src=\"https://storage.googleapis.com/download.tensorflow.org/models/tflite/screenshots/model_maker_squad_showcase.png\"  width=\"500\"></p>\n",
        "\n",
        "<p align=\"center\">\n",
        "    <em>Answers are spans in the passage (image credit: <a href=\"https://rajpurkar.github.io/mlx/qa-and-squad/\">SQuAD blog</a>) </em>\n",
        "</p>\n",
        "\n",
        "As for the model of question answer task, the inputs should be the passage and question pair that are already preprocessed, the outputs should be the start logits and end logits for each token in the passage.\n",
        "The size of input could be set and adjusted according to the length of passage and question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gb7P4WQta8Ub"
      },
      "source": [
        "## End-to-End Overview\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7cIHjIfbDlG"
      },
      "source": [
        "The following code snippet demonstrates how to get the model within a few lines of code. The overall process includes 5 steps: (1) choose a model, (2) load data, (3) retrain the model, (4) evaluate, and (5) export it to TensorFlow Lite format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQPdlxZBYuZG"
      },
      "source": [
        "```python\n",
        "# Chooses a model specification that represents the model.\n",
        "spec = model_spec.get('mobilebert_qa')\n",
        "\n",
        "# Gets the training data and validation data.\n",
        "train_data = DataLoader.from_squad(train_data_path, spec, is_training=True)\n",
        "validation_data = DataLoader.from_squad(validation_data_path, spec, is_training=False)\n",
        "\n",
        "# Fine-tunes the model.\n",
        "model = question_answer.create(train_data, model_spec=spec)\n",
        "\n",
        "# Gets the evaluation result.\n",
        "metric = model.evaluate(validation_data)\n",
        "\n",
        "# Exports the model to the TensorFlow Lite format with metadata in the export directory.\n",
        "model.export(export_dir)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exScAdvBbNEi"
      },
      "source": [
        "The following sections explain the code in more detail."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scann --no-dependencies\n",
        "!pip install numpy==1.23.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HjXJUOb2Vv0j",
        "outputId": "7fa80345-8e30-4a5a-cd79-bd08ca59be88"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scann\n",
            "  Downloading scann-1.4.0-cp311-cp311-manylinux_2_27_x86_64.whl.metadata (5.8 kB)\n",
            "Downloading scann-1.4.0-cp311-cp311-manylinux_2_27_x86_64.whl (11.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scann\n",
            "Successfully installed scann-1.4.0\n",
            "Collecting numpy==1.23.0\n",
            "  Downloading numpy-1.23.0.tar.gz (10.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: numpy\n",
            "  Building wheel for numpy (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for numpy: filename=numpy-1.23.0-cp311-cp311-linux_x86_64.whl size=19730007 sha256=e57f1ab886e180ee4b6aefa8be4c316f35c05dd2bcbc1364255a20e43bd27aba\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/36/1a/3ec6b85008bea3151efb003f5d41baa7bf4966cb43c1c2470b\n",
            "Successfully built numpy\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "scann 1.4.0 requires numpy~=2.0, but you have numpy 1.23.0 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.23.0 which is incompatible.\n",
            "dm-tree 0.1.9 requires numpy>=1.23.3; python_version >= \"3.11\", but you have numpy 1.23.0 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.0 which is incompatible.\n",
            "pandas 2.2.2 requires numpy>=1.23.2; python_version == \"3.11\", but you have numpy 1.23.0 which is incompatible.\n",
            "scipy 1.14.1 requires numpy<2.3,>=1.23.5, but you have numpy 1.23.0 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.23.0 which is incompatible.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.0 which is incompatible.\n",
            "bigframes 1.42.0 requires numpy>=1.24.0, but you have numpy 1.23.0 which is incompatible.\n",
            "flax 0.10.5 requires numpy>=1.23.2; python_version >= \"3.11\", but you have numpy 1.23.0 which is incompatible.\n",
            "albumentations 2.0.5 requires numpy>=1.24.4, but you have numpy 1.23.0 which is incompatible.\n",
            "plotnine 0.14.5 requires numpy>=1.23.5, but you have numpy 1.23.0 which is incompatible.\n",
            "ml-dtypes 0.4.1 requires numpy>=1.23.3; python_version >= \"3.11\", but you have numpy 1.23.0 which is incompatible.\n",
            "blosc2 3.3.0 requires numpy>=1.26, but you have numpy 1.23.0 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.23.0 which is incompatible.\n",
            "albucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.23.0 which is incompatible.\n",
            "mizani 0.13.3 requires numpy>=1.23.5, but you have numpy 1.23.0 which is incompatible.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.0 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.0 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.23.0 which is incompatible.\n",
            "astropy 7.0.1 requires numpy>=1.23.2, but you have numpy 1.23.0 which is incompatible.\n",
            "opencv-python 4.11.0.86 requires numpy>=1.23.5; python_version >= \"3.11\", but you have numpy 1.23.0 which is incompatible.\n",
            "pandas-stubs 2.2.2.240909 requires numpy>=1.23.5, but you have numpy 1.23.0 which is incompatible.\n",
            "xarray 2025.1.2 requires numpy>=1.24, but you have numpy 1.23.0 which is incompatible.\n",
            "opencv-contrib-python 4.11.0.86 requires numpy>=1.23.5; python_version >= \"3.11\", but you have numpy 1.23.0 which is incompatible.\n",
            "pymc 5.21.2 requires numpy>=1.25.0, but you have numpy 1.23.0 which is incompatible.\n",
            "opencv-python-headless 4.11.0.86 requires numpy>=1.23.5; python_version >= \"3.11\", but you have numpy 1.23.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.23.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "eac9855c92ec406fa0c599b4f440e4e7"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements_nightly.txt -c constraints.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12aVFEwzVrvZ",
        "outputId": "75dd2c56-4d7b-4120-d586-7206c88a50a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ignoring tensorflow-hub: markers 'python_version < \"3\"' don't match your environment\n",
            "Collecting tf-models-official==2.3.0 (from -r requirements_nightly.txt (line 3))\n",
            "  Using cached tf_models_official-2.3.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting tensorflow-hub<0.13,>=0.7.0 (from -r requirements_nightly.txt (line 6))\n",
            "  Using cached tensorflow_hub-0.12.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting numpy==1.23.5 (from -r requirements_nightly.txt (line 7))\n",
            "  Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: pillow>=7.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements_nightly.txt (line 8)) (11.1.0)\n",
            "Requirement already satisfied: sentencepiece>=0.1.91 in /usr/local/lib/python3.11/dist-packages (from -r requirements_nightly.txt (line 9)) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-datasets>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements_nightly.txt (line 10)) (4.9.8)\n",
            "Collecting fire>=0.3.1 (from -r requirements_nightly.txt (line 11))\n",
            "  Using cached fire-0.7.0.tar.gz (87 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements_nightly.txt (line 12)) (25.2.10)\n",
            "Requirement already satisfied: absl-py>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements_nightly.txt (line 13)) (1.4.0)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from -r requirements_nightly.txt (line 14))\n",
            "  Using cached urllib3-1.25.11-py2.py3-none-any.whl.metadata (41 kB)\n",
            "Collecting tflite-support>=0.4.2 (from -r requirements_nightly.txt (line 15))\n",
            "  Using cached tflite_support-0.4.4-cp311-cp311-manylinux2014_x86_64.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: tensorflow>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements_nightly.txt (line 16)) (2.15.1)\n",
            "Requirement already satisfied: numba>=0.53 in /usr/local/lib/python3.11/dist-packages (from -r requirements_nightly.txt (line 19)) (0.60.0)\n",
            "Requirement already satisfied: librosa>=0.8.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements_nightly.txt (line 20)) (0.11.0)\n",
            "Requirement already satisfied: lxml>=4.6.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements_nightly.txt (line 21)) (5.3.2)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements_nightly.txt (line 22)) (6.0.2)\n",
            "Collecting matplotlib<3.5.0,>=3.0.3 (from -r requirements_nightly.txt (line 24))\n",
            "  Using cached matplotlib-3.4.3.tar.gz (37.9 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements_nightly.txt (line 25)) (1.17.0)\n",
            "Collecting tfa-nightly (from -r requirements_nightly.txt (line 26))\n",
            "  Using cached tfa_nightly-0.23.0.dev20240415222534-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting neural-structured-learning>=1.3.1 (from -r requirements_nightly.txt (line 27))\n",
            "  Using cached neural_structured_learning-1.4.0-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting tensorflow-model-optimization>=0.5 (from -r requirements_nightly.txt (line 28))\n",
            "  Using cached tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl.metadata (904 bytes)\n",
            "Requirement already satisfied: Cython>=0.29.13 in /usr/local/lib/python3.11/dist-packages (from -r requirements_nightly.txt (line 29)) (3.0.12)\n",
            "Collecting scann==1.3.0 (from -r requirements_nightly.txt (line 30))\n",
            "  Using cached scann-1.3.0-cp311-cp311-manylinux_2_27_x86_64.whl.metadata (4.4 kB)\n",
            "Collecting tensorflowjs<3.19.0,>=2.4.0 (from -r requirements_nightly.txt (line 31))\n",
            "  Using cached tensorflowjs-3.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting dataclasses (from tf-models-official==2.3.0->-r requirements_nightly.txt (line 3))\n",
            "  Using cached dataclasses-0.6-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.11/dist-packages (from tf-models-official==2.3.0->-r requirements_nightly.txt (line 3)) (0.5.0)\n",
            "Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.11/dist-packages (from tf-models-official==2.3.0->-r requirements_nightly.txt (line 3)) (2.164.0)\n",
            "Requirement already satisfied: google-cloud-bigquery>=0.31.0 in /usr/local/lib/python3.11/dist-packages (from tf-models-official==2.3.0->-r requirements_nightly.txt (line 3)) (3.31.0)\n",
            "Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.11/dist-packages (from tf-models-official==2.3.0->-r requirements_nightly.txt (line 3)) (1.7.4.2)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.11/dist-packages (from tf-models-official==2.3.0->-r requirements_nightly.txt (line 3)) (4.11.0.86)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from tf-models-official==2.3.0->-r requirements_nightly.txt (line 3)) (2.2.2)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.11/dist-packages (from tf-models-official==2.3.0->-r requirements_nightly.txt (line 3)) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from tf-models-official==2.3.0->-r requirements_nightly.txt (line 3)) (9.0.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from tf-models-official==2.3.0->-r requirements_nightly.txt (line 3)) (1.14.1)\n",
            "Collecting tensorflow-addons (from tf-models-official==2.3.0->-r requirements_nightly.txt (line 3))\n",
            "  Using cached tensorflow_addons-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: tf-slim>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tf-models-official==2.3.0->-r requirements_nightly.txt (line 3)) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-hub<0.13,>=0.7.0->-r requirements_nightly.txt (line 6)) (4.25.6)\n",
            "Requirement already satisfied: array_record>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets>=2.1.0->-r requirements_nightly.txt (line 10)) (0.7.1)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets>=2.1.0->-r requirements_nightly.txt (line 10)) (0.1.9)\n",
            "Requirement already satisfied: etils>=1.9.1 in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets>=2.1.0->-r requirements_nightly.txt (line 10)) (1.12.2)\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets>=2.1.0->-r requirements_nightly.txt (line 10)) (4.2.1)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets>=2.1.0->-r requirements_nightly.txt (line 10)) (2.3)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets>=2.1.0->-r requirements_nightly.txt (line 10)) (18.1.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets>=2.1.0->-r requirements_nightly.txt (line 10)) (2.32.3)\n",
            "Requirement already satisfied: simple_parsing in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets>=2.1.0->-r requirements_nightly.txt (line 10)) (0.1.7)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets>=2.1.0->-r requirements_nightly.txt (line 10)) (1.17.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets>=2.1.0->-r requirements_nightly.txt (line 10)) (3.0.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets>=2.1.0->-r requirements_nightly.txt (line 10)) (0.10.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets>=2.1.0->-r requirements_nightly.txt (line 10)) (4.67.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets>=2.1.0->-r requirements_nightly.txt (line 10)) (1.14.1)\n",
            "Collecting protobuf>=3.8.0 (from tensorflow-hub<0.13,>=0.7.0->-r requirements_nightly.txt (line 6))\n",
            "  Using cached protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "Collecting sounddevice>=0.4.4 (from tflite-support>=0.4.2->-r requirements_nightly.txt (line 15))\n",
            "  Using cached sounddevice-0.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting pybind11>=2.6.0 (from tflite-support>=0.4.2->-r requirements_nightly.txt (line 15))\n",
            "  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.6.0->-r requirements_nightly.txt (line 16)) (1.6.3)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.6.0->-r requirements_nightly.txt (line 16)) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.6.0->-r requirements_nightly.txt (line 16)) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.6.0->-r requirements_nightly.txt (line 16)) (3.13.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.6.0->-r requirements_nightly.txt (line 16)) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.3.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.6.0->-r requirements_nightly.txt (line 16)) (0.3.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.6.0->-r requirements_nightly.txt (line 16)) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.6.0->-r requirements_nightly.txt (line 16)) (24.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.6.0->-r requirements_nightly.txt (line 16)) (75.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.6.0->-r requirements_nightly.txt (line 16)) (4.13.2)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.6.0->-r requirements_nightly.txt (line 16)) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.6.0->-r requirements_nightly.txt (line 16)) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.6.0->-r requirements_nightly.txt (line 16)) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.6.0->-r requirements_nightly.txt (line 16)) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.6.0->-r requirements_nightly.txt (line 16)) (2.15.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.53->-r requirements_nightly.txt (line 19)) (0.43.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.8.1->-r requirements_nightly.txt (line 20)) (3.0.1)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.8.1->-r requirements_nightly.txt (line 20)) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.8.1->-r requirements_nightly.txt (line 20)) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.8.1->-r requirements_nightly.txt (line 20)) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.8.1->-r requirements_nightly.txt (line 20)) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.8.1->-r requirements_nightly.txt (line 20)) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.8.1->-r requirements_nightly.txt (line 20)) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.8.1->-r requirements_nightly.txt (line 20)) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.8.1->-r requirements_nightly.txt (line 20)) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<3.5.0,>=3.0.3->-r requirements_nightly.txt (line 24)) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<3.5.0,>=3.0.3->-r requirements_nightly.txt (line 24)) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<3.5.0,>=3.0.3->-r requirements_nightly.txt (line 24)) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib<3.5.0,>=3.0.3->-r requirements_nightly.txt (line 24)) (2.8.2)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tfa-nightly->-r requirements_nightly.txt (line 26))\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.11/dist-packages (from neural-structured-learning>=1.3.1->-r requirements_nightly.txt (line 27)) (25.3.0)\n",
            "Collecting packaging (from tensorflow>=2.6.0->-r requirements_nightly.txt (line 16))\n",
            "  Downloading packaging-20.9-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow>=2.6.0->-r requirements_nightly.txt (line 16)) (0.45.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets>=2.1.0->-r requirements_nightly.txt (line 10)) (0.8.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets>=2.1.0->-r requirements_nightly.txt (line 10)) (2025.3.2)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets>=2.1.0->-r requirements_nightly.txt (line 10)) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets>=2.1.0->-r requirements_nightly.txt (line 10)) (3.21.0)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.3.0->-r requirements_nightly.txt (line 3)) (0.22.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.3.0->-r requirements_nightly.txt (line 3)) (2.38.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.3.0->-r requirements_nightly.txt (line 3)) (0.2.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.3.0->-r requirements_nightly.txt (line 3)) (2.24.2)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.3.0->-r requirements_nightly.txt (line 3)) (4.1.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery>=0.31.0->tf-models-official==2.3.0->-r requirements_nightly.txt (line 3)) (2.4.3)\n",
            "Requirement already satisfied: google-resumable-media<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery>=0.31.0->tf-models-official==2.3.0->-r requirements_nightly.txt (line 3)) (2.7.2)\n",
            "INFO: pip is looking at multiple versions of google-cloud-bigquery to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting google-cloud-bigquery>=0.31.0 (from tf-models-official==2.3.0->-r requirements_nightly.txt (line 3))\n",
            "  Downloading google_cloud_bigquery-3.30.0-py2.py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle>=1.3.9->tf-models-official==2.3.0->-r requirements_nightly.txt (line 3)) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle>=1.3.9->tf-models-official==2.3.0->-r requirements_nightly.txt (line 3)) (2025.1.31)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle>=1.3.9->tf-models-official==2.3.0->-r requirements_nightly.txt (line 3)) (3.4.1)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle>=1.3.9->tf-models-official==2.3.0->-r requirements_nightly.txt (line 3)) (3.10)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle>=1.3.9->tf-models-official==2.3.0->-r requirements_nightly.txt (line 3)) (8.0.4)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle>=1.3.9->tf-models-official==2.3.0->-r requirements_nightly.txt (line 3)) (1.3)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle>=1.3.9->tf-models-official==2.3.0->-r requirements_nightly.txt (line 3)) (0.5.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.22.0->tf-models-official==2.3.0->-r requirements_nightly.txt (line 3)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.22.0->tf-models-official==2.3.0->-r requirements_nightly.txt (line 3)) (2025.2)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa>=0.8.1->-r requirements_nightly.txt (line 20)) (4.3.7)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa>=0.8.1->-r requirements_nightly.txt (line 20)) (3.6.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice>=0.4.4->tflite-support>=0.4.2->-r requirements_nightly.txt (line 15)) (1.17.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.6.0->-r requirements_nightly.txt (line 16)) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.6.0->-r requirements_nightly.txt (line 16)) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.6.0->-r requirements_nightly.txt (line 16)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.6.0->-r requirements_nightly.txt (line 16)) (3.1.3)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.11/dist-packages (from simple_parsing->tensorflow-datasets>=2.1.0->-r requirements_nightly.txt (line 10)) (0.16)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.56.4 in /usr/local/lib/python3.11/dist-packages (from tensorflow-metadata->tensorflow-datasets>=2.1.0->-r requirements_nightly.txt (line 10)) (1.70.0)\n",
            "INFO: pip is looking at multiple versions of tensorflow-metadata to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tensorflow-metadata (from tensorflow-datasets>=2.1.0->-r requirements_nightly.txt (line 10))\n",
            "  Downloading tensorflow_metadata-1.17.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading tensorflow_metadata-1.16.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "  Downloading tensorflow_metadata-1.16.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "  Downloading tensorflow_metadata-1.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "  Downloading tensorflow_metadata-1.14.0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->tflite-support>=0.4.2->-r requirements_nightly.txt (line 15)) (2.22)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official==2.3.0->-r requirements_nightly.txt (line 3)) (1.26.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery>=0.31.0->tf-models-official==2.3.0->-r requirements_nightly.txt (line 3)) (1.71.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client>=1.6.7->tf-models-official==2.3.0->-r requirements_nightly.txt (line 3)) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client>=1.6.7->tf-models-official==2.3.0->-r requirements_nightly.txt (line 3)) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client>=1.6.7->tf-models-official==2.3.0->-r requirements_nightly.txt (line 3)) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=2.6.0->-r requirements_nightly.txt (line 16)) (2.0.0)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.11/dist-packages (from google-resumable-media<3.0.0,>=2.0.0->google-cloud-bigquery>=0.31.0->tf-models-official==2.3.0->-r requirements_nightly.txt (line 3)) (1.7.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow>=2.6.0->-r requirements_nightly.txt (line 16)) (3.0.2)\n",
            "INFO: pip is looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery>=0.31.0->tf-models-official==2.3.0->-r requirements_nightly.txt (line 3))\n",
            "  Downloading grpcio_status-1.70.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.69.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.68.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.68.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.67.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.67.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.66.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "INFO: pip is still looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading grpcio_status-1.66.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.66.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.65.5-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.65.4-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.65.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading grpcio_status-1.65.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.64.3-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.64.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.64.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.63.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.63.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.62.3-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading grpcio_status-1.62.2-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading grpcio_status-1.62.1-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading grpcio_status-1.62.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading grpcio_status-1.61.3-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading grpcio_status-1.60.2-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading grpcio_status-1.60.1-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading grpcio_status-1.60.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading grpcio_status-1.59.5-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading grpcio_status-1.59.3-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading grpcio_status-1.59.2-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading grpcio_status-1.59.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading grpcio_status-1.58.3-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading grpcio_status-1.58.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading grpcio_status-1.57.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "  Downloading grpcio_status-1.56.2-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading grpcio_status-1.56.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading grpcio_status-1.55.3-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading grpcio_status-1.54.3-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading grpcio_status-1.54.2-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading grpcio_status-1.54.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading grpcio_status-1.53.2-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading grpcio_status-1.53.1-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading grpcio_status-1.53.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading grpcio_status-1.51.3-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading grpcio_status-1.51.1-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading grpcio_status-1.50.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading grpcio_status-1.49.1-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting googleapis-common-protos<2,>=1.52.0 (from tensorflow-metadata->tensorflow-datasets>=2.1.0->-r requirements_nightly.txt (line 10))\n",
            "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
            "  Downloading googleapis_common_protos-1.69.2-py3-none-any.whl.metadata (9.3 kB)\n",
            "  Downloading googleapis_common_protos-1.69.1-py2.py3-none-any.whl.metadata (9.3 kB)\n",
            "  Downloading googleapis_common_protos-1.69.0-py2.py3-none-any.whl.metadata (5.1 kB)\n",
            "  Downloading googleapis_common_protos-1.68.0-py2.py3-none-any.whl.metadata (5.1 kB)\n",
            "  Downloading googleapis_common_protos-1.67.0-py2.py3-none-any.whl.metadata (5.1 kB)\n",
            "  Downloading googleapis_common_protos-1.66.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "  Downloading googleapis_common_protos-1.65.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "  Downloading googleapis_common_protos-1.64.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "  Downloading googleapis_common_protos-1.63.2-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "  Downloading googleapis_common_protos-1.63.1-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "  Downloading googleapis_common_protos-1.63.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "  Downloading googleapis_common_protos-1.62.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "  Downloading googleapis_common_protos-1.61.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "  Downloading googleapis_common_protos-1.60.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "  Downloading googleapis_common_protos-1.59.1-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "  Downloading googleapis_common_protos-1.59.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "  Downloading googleapis_common_protos-1.58.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "  Downloading googleapis_common_protos-1.57.1-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "  Downloading googleapis_common_protos-1.57.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "  Downloading googleapis_common_protos-1.56.4-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading googleapis_common_protos-1.56.3-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading googleapis_common_protos-1.56.2-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting google-cloud-core<3.0.0dev,>=2.4.1 (from google-cloud-bigquery>=0.31.0->tf-models-official==2.3.0->-r requirements_nightly.txt (line 3))\n",
            "  Downloading google_cloud_core-2.4.3-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting google-api-core[grpc]<3.0.0dev,>=2.11.1 (from google-cloud-bigquery>=0.31.0->tf-models-official==2.3.0->-r requirements_nightly.txt (line 3))\n",
            "  Downloading google_api_core-2.24.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "  Downloading google_api_core-2.24.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "  Downloading google_api_core-2.24.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "  Downloading google_api_core-2.23.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "  Downloading google_api_core-2.22.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "  Downloading google_api_core-2.21.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "  Downloading google_api_core-2.20.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "  Downloading google_api_core-2.19.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "  Downloading google_api_core-2.19.1-py3-none-any.whl.metadata (2.7 kB)\n",
            "  Downloading google_api_core-2.19.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "  Downloading google_api_core-2.18.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "  Downloading google_api_core-2.17.1-py3-none-any.whl.metadata (2.7 kB)\n",
            "  Downloading google_api_core-2.17.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "  Downloading google_api_core-2.16.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "  Downloading google_api_core-2.16.1-py3-none-any.whl.metadata (2.7 kB)\n",
            "  Downloading google_api_core-2.16.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "  Downloading google_api_core-2.15.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "  Downloading google_api_core-2.14.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading google_api_core-2.13.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading google_api_core-2.13.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "  Downloading google_api_core-2.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "  Downloading google_api_core-2.11.1-py3-none-any.whl.metadata (2.7 kB)\n",
            "INFO: pip is looking at multiple versions of google-api-core[grpc] to determine which version is compatible with other requirements. This could take a while.\n",
            "INFO: pip is still looking at multiple versions of google-api-core[grpc] to determine which version is compatible with other requirements. This could take a while.\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "Collecting google-cloud-bigquery>=0.31.0 (from tf-models-official==2.3.0->-r requirements_nightly.txt (line 3))\n",
            "  Downloading google_cloud_bigquery-3.29.0-py2.py3-none-any.whl.metadata (7.6 kB)\n",
            "INFO: pip is still looking at multiple versions of tensorflow-metadata to determine which version is compatible with other requirements. This could take a while.\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "Collecting tensorflow-metadata (from tensorflow-datasets>=2.1.0->-r requirements_nightly.txt (line 10))\n",
            "  Downloading tensorflow_metadata-1.13.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "  Downloading tensorflow_metadata-1.13.0-py3-none-any.whl.metadata (2.1 kB)\n",
            "  Downloading tensorflow_metadata-1.12.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Downloading tensorflow_metadata-1.11.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Downloading tensorflow_metadata-1.10.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Downloading tensorflow_metadata-1.9.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Downloading tensorflow_metadata-1.8.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Downloading tensorflow_metadata-1.7.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Downloading tensorflow_metadata-1.6.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Downloading tensorflow_metadata-1.5.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Downloading tensorflow_metadata-1.4.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Downloading tensorflow_metadata-1.2.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Downloading tensorflow_metadata-1.1.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Downloading tensorflow_metadata-1.0.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Downloading tensorflow_metadata-0.30.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Downloading tensorflow_metadata-0.29.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Downloading tensorflow_metadata-0.28.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Downloading tensorflow_metadata-0.27.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Downloading tensorflow_metadata-0.26.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Downloading tensorflow_metadata-0.25.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Downloading tensorflow_metadata-0.24.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Downloading tensorflow_metadata-0.23.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Downloading tensorflow_metadata-0.22.2-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "  Downloading tensorflow_metadata-0.22.1-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "  Downloading tensorflow_metadata-0.22.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "  Downloading tensorflow_metadata-0.21.2-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "  Downloading tensorflow_metadata-0.21.1-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "  Downloading tensorflow_metadata-0.21.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "  Downloading tensorflow_metadata-0.15.2-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "  Downloading tensorflow_metadata-0.15.1-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "  Downloading tensorflow_metadata-0.15.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "  Downloading tensorflow_metadata-0.14.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "  Downloading tensorflow_metadata-0.13.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Downloading tensorflow_metadata-0.12.1-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting tf-slim>=1.1.0 (from tf-models-official==2.3.0->-r requirements_nightly.txt (line 3))\n",
            "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl.metadata (1.6 kB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NJss8GRg1yOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall scann==1.4.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rt0clNmjVsaG",
        "outputId": "440ca62a-fd7b-424e-d672-a62acb1e268f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping scann as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall tflite-model-make-nightly"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqRH6IfuTGGu",
        "outputId": "0751d27b-ae87-4502-9957-d74f5df9923e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping tflite-model-make-nightly as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "qhl8lqVamEty",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "269568e2-204f-4052-bf5f-c3aa4a105a0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libportaudio2 is already the newest version (19.6.0-1.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n",
            "Collecting tflite-model-maker-nightly==0.4.4.dev202402230613\n",
            "  Using cached tflite_model_maker_nightly-0.4.4.dev202402230613-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting tf-models-official==2.3.0 (from tflite-model-maker-nightly==0.4.4.dev202402230613)\n",
            "  Using cached tf_models_official-2.3.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting numpy<1.23.4,>=1.17.3 (from tflite-model-maker-nightly==0.4.4.dev202402230613)\n",
            "  Using cached numpy-1.23.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Collecting pillow>=7.0.0 (from tflite-model-maker-nightly==0.4.4.dev202402230613)\n",
            "  Using cached pillow-11.2.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
            "Collecting sentencepiece>=0.1.91 (from tflite-model-maker-nightly==0.4.4.dev202402230613)\n",
            "  Using cached sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting tensorflow-datasets>=2.1.0 (from tflite-model-maker-nightly==0.4.4.dev202402230613)\n",
            "  Using cached tensorflow_datasets-4.9.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting fire>=0.3.1 (from tflite-model-maker-nightly==0.4.4.dev202402230613)\n",
            "  Using cached fire-0.7.0.tar.gz (87 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting flatbuffers>=2.0 (from tflite-model-maker-nightly==0.4.4.dev202402230613)\n",
            "  Using cached flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Collecting absl-py>=0.10.0 (from tflite-model-maker-nightly==0.4.4.dev202402230613)\n",
            "  Using cached absl_py-2.2.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from tflite-model-maker-nightly==0.4.4.dev202402230613)\n",
            "  Using cached urllib3-1.25.11-py2.py3-none-any.whl.metadata (41 kB)\n",
            "Collecting tflite-support>=0.4.2 (from tflite-model-maker-nightly==0.4.4.dev202402230613)\n",
            "  Using cached tflite_support-0.4.4-cp311-cp311-manylinux2014_x86_64.whl.metadata (2.4 kB)\n",
            "Collecting tensorflow>=2.6.0 (from tflite-model-maker-nightly==0.4.4.dev202402230613)\n",
            "  Using cached tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting numba>=0.53 (from tflite-model-maker-nightly==0.4.4.dev202402230613)\n",
            "  Using cached numba-0.61.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
            "Collecting librosa>=0.8.1 (from tflite-model-maker-nightly==0.4.4.dev202402230613)\n",
            "  Using cached librosa-0.11.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting lxml>=4.6.1 (from tflite-model-maker-nightly==0.4.4.dev202402230613)\n",
            "  Using cached lxml-5.3.2-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting PyYAML>=5.1 (from tflite-model-maker-nightly==0.4.4.dev202402230613)\n",
            "  Using cached PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting matplotlib<3.5.0,>=3.0.3 (from tflite-model-maker-nightly==0.4.4.dev202402230613)\n",
            "  Using cached matplotlib-3.4.3.tar.gz (37.9 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tflite-model-maker-nightly==0.4.4.dev202402230613) (1.16.0)\n",
            "Collecting tfa-nightly (from tflite-model-maker-nightly==0.4.4.dev202402230613)\n",
            "  Using cached tfa_nightly-0.23.0.dev20240415222534-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting neural-structured-learning>=1.3.1 (from tflite-model-maker-nightly==0.4.4.dev202402230613)\n",
            "  Using cached neural_structured_learning-1.4.0-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting tensorflow-model-optimization>=0.5 (from tflite-model-maker-nightly==0.4.4.dev202402230613)\n",
            "  Using cached tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl.metadata (904 bytes)\n",
            "Collecting Cython>=0.29.13 (from tflite-model-maker-nightly==0.4.4.dev202402230613)\n",
            "  Using cached Cython-3.0.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
            "INFO: pip is looking at multiple versions of tflite-model-maker-nightly to determine which version is compatible with other requirements. This could take a while.\n",
            "\u001b[31mERROR: Ignored the following versions that require a different python version: 0.52.0 Requires-Python >=3.6,<3.9; 0.52.0rc3 Requires-Python >=3.6,<3.9; 0.53.0 Requires-Python >=3.6,<3.10; 0.53.0rc1.post1 Requires-Python >=3.6,<3.10; 0.53.0rc2 Requires-Python >=3.6,<3.10; 0.53.0rc3 Requires-Python >=3.6,<3.10; 0.53.1 Requires-Python >=3.6,<3.10; 0.54.0 Requires-Python >=3.7,<3.10; 0.54.0rc2 Requires-Python >=3.7,<3.10; 0.54.0rc3 Requires-Python >=3.7,<3.10; 0.54.1 Requires-Python >=3.7,<3.10; 0.55.0 Requires-Python >=3.7,<3.11; 0.55.0rc1 Requires-Python >=3.7,<3.11; 0.55.1 Requires-Python >=3.7,<3.11; 0.55.2 Requires-Python >=3.7,<3.11; 1.21.2 Requires-Python >=3.7,<3.11; 1.21.3 Requires-Python >=3.7,<3.11; 1.21.4 Requires-Python >=3.7,<3.11; 1.21.5 Requires-Python >=3.7,<3.11; 1.21.6 Requires-Python >=3.7,<3.11\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement scann==1.2.6 (from tflite-model-maker-nightly) (from versions: 1.2.10, 1.3.0, 1.3.1, 1.3.2, 1.3.3, 1.3.4, 1.3.5, 1.4.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for scann==1.2.6\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!sudo apt -y install libportaudio2\n",
        "!pip install tflite-model-maker-nightly==0.4.4.dev202402230613"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcLF2PKkSbV3"
      },
      "source": [
        "## Prerequisites\n",
        "\n",
        "To run this example, install the required packages, including the Model Maker package from the [GitHub repo](https://github.com/tensorflow/examples/tree/master/tensorflow_examples/lite/model_maker)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6lRhVK9Q_0U"
      },
      "source": [
        "Import the required packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtxiUeZEiXpt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2')\n",
        "\n",
        "from tflite_model_maker import model_spec\n",
        "from tflite_model_maker import question_answer\n",
        "from tflite_model_maker.config import ExportFormat\n",
        "from tflite_model_maker.question_answer import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l65ctmtW7_FF"
      },
      "source": [
        "The \"End-to-End Overview\" demonstrates a simple end-to-end example. The following sections walk through the example step by step to show more detail."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJ_B8fMDOhMR"
      },
      "source": [
        "## Choose a model_spec that represents a model for question answer\n",
        "\n",
        "Each `model_spec` object represents a specific model for question answer. The Model Maker currently supports MobileBERT and BERT-Base models.\n",
        "\n",
        "Supported Model | Name of model_spec | Model Description\n",
        "--- | --- | ---\n",
        "[MobileBERT](https://arxiv.org/pdf/2004.02984.pdf)  | 'mobilebert_qa' | 4.3x smaller and 5.5x faster than BERT-Base while achieving competitive results, suitable for on-device scenario.\n",
        "[MobileBERT-SQuAD](https://arxiv.org/pdf/2004.02984.pdf)  | 'mobilebert_qa_squad' | Same model architecture as MobileBERT model and the initial model is already retrained on [SQuAD1.1](https://rajpurkar.github.io/SQuAD-explorer/).\n",
        "[BERT-Base](https://arxiv.org/pdf/1810.04805.pdf) | 'bert_qa' | Standard BERT model that widely used in NLP tasks.\n",
        "\n",
        "In this tutorial, [MobileBERT-SQuAD](https://arxiv.org/pdf/2004.02984.pdf) is used as an example. Since the model is already retrained on [SQuAD1.1](https://rajpurkar.github.io/SQuAD-explorer/), it could coverage faster for question answer task.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEAWuZQ1PFiX"
      },
      "outputs": [],
      "source": [
        "spec = model_spec.get('mobilebert_qa_squad')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygEncJxtl-nQ"
      },
      "source": [
        "## Load Input Data Specific to an On-device ML App and Preprocess the Data\n",
        "\n",
        "The [TriviaQA](https://nlp.cs.washington.edu/triviaqa/) is a reading comprehension dataset containing over 650K question-answer-evidence triples. In this tutorial, you will use a subset of this dataset to learn how to use the Model Maker library.\n",
        "\n",
        "To load the data, convert the TriviaQA dataset to the [SQuAD1.1](https://rajpurkar.github.io/SQuAD-explorer/) format by running the [converter Python script](https://github.com/mandarjoshi90/triviaqa#miscellaneous) with `--sample_size=8000` and a set of `web` data. Modify the conversion code a little bit by:\n",
        "* Skipping the samples that couldn't find any answer in the context document;\n",
        "* Getting the original answer in the context without uppercase or lowercase.\n",
        "\n",
        "Download the archived version of the already converted dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tOfUr2KlgpU"
      },
      "outputs": [],
      "source": [
        "train_data_path = tf.keras.utils.get_file(\n",
        "    fname='triviaqa-web-train-8000.json',\n",
        "    origin='https://storage.googleapis.com/download.tensorflow.org/models/tflite/dataset/triviaqa-web-train-8000.json')\n",
        "validation_data_path = tf.keras.utils.get_file(\n",
        "    fname='triviaqa-verified-web-dev.json',\n",
        "    origin='https://storage.googleapis.com/download.tensorflow.org/models/tflite/dataset/triviaqa-verified-web-dev.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfZk8GNr_1nc"
      },
      "source": [
        "You can also train the MobileBERT model with your own dataset. If you are running this notebook on Colab, upload your data by using the left sidebar.\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/download.tensorflow.org/models/tflite/screenshots/model_maker_question_answer.png\" alt=\"Upload File\" width=\"800\" hspace=\"100\">\n",
        "\n",
        "If you prefer not to upload your data to the cloud, you can also run the library offline by following the [guide](https://github.com/tensorflow/examples/tree/master/tensorflow_examples/lite/model_maker)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E051HBUM5owi"
      },
      "source": [
        "Use the `DataLoader.from_squad` method to load and preprocess the [SQuAD format](https://rajpurkar.github.io/SQuAD-explorer/) data according to a specific `model_spec`. You can use either SQuAD2.0 or SQuAD1.1 formats. Setting parameter `version_2_with_negative` as `True` means the formats is SQuAD2.0. Otherwise, the format is SQuAD1.1. By default, `version_2_with_negative` is `False`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_fOlZsklmlL"
      },
      "outputs": [],
      "source": [
        "train_data = DataLoader.from_squad(train_data_path, spec, is_training=True)\n",
        "validation_data = DataLoader.from_squad(validation_data_path, spec, is_training=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWuoensX4vDA"
      },
      "source": [
        "## Customize the TensorFlow Model\n",
        "\n",
        "Create a custom question answer model based on the loaded data. The `create` function comprises the following steps:\n",
        "\n",
        "1. Creates the model for question answer according to `model_spec`.\n",
        "2. Train the question answer model. The default epochs and the default batch size are set according to two variables `default_training_epochs` and `default_batch_size` in the `model_spec` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvYSUuJY3QxR"
      },
      "outputs": [],
      "source": [
        "model = question_answer.create(train_data, model_spec=spec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JKI-pNc8idH"
      },
      "source": [
        "Have a look at the detailed model structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gd7Hs8TF8n3H"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LP5FPk_tOxoZ"
      },
      "source": [
        "## Evaluate the Customized Model\n",
        "\n",
        "Evaluate the model on the validation data and get a dict of metrics including `f1` score and `exact match` etc. Note that metrics are different for SQuAD1.1 and SQuAD2.0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8c2ZQ0J3Riy"
      },
      "outputs": [],
      "source": [
        "model.evaluate(validation_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeHoGAceO2xV"
      },
      "source": [
        "## Export to TensorFlow Lite Model\n",
        "\n",
        "Convert the trained model to TensorFlow Lite model format with [metadata](https://www.tensorflow.org/lite/models/convert/metadata) so that you can later use in an on-device ML application. The vocab file are embedded in metadata. The default TFLite filename is `model.tflite`.\n",
        "\n",
        "In many on-device ML application, the model size is an important factor. Therefore, it is recommended that you apply quantize the model to make it smaller and potentially run faster.\n",
        "The default post-training quantization technique is dynamic range quantization for the BERT and MobileBERT models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Im6wA9lK3TQB"
      },
      "outputs": [],
      "source": [
        "model.export(export_dir='.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w12kvDdHJIGH"
      },
      "source": [
        "You can use the TensorFlow Lite model file in the [bert_qa](https://github.com/tensorflow/examples/tree/master/lite/examples/bert_qa/android) reference app using [BertQuestionAnswerer API](https://www.tensorflow.org/lite/inference_with_metadata/task_library/bert_question_answerer) in [TensorFlow Lite Task Library](https://www.tensorflow.org/lite/inference_with_metadata/task_library/overview) by downloading it from the left sidebar on Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFnJPvq3VGh3"
      },
      "source": [
        "The allowed export formats can be one or a list of the following:\n",
        "\n",
        "*   `ExportFormat.TFLITE`\n",
        "*   `ExportFormat.VOCAB`\n",
        "*   `ExportFormat.SAVED_MODEL`\n",
        "\n",
        "By default, it just exports TensorFlow Lite model with metadata. You can also selectively export different files. For instance, exporting only the vocab file as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ro2hz4kXVImY"
      },
      "outputs": [],
      "source": [
        "model.export(export_dir='.', export_format=ExportFormat.VOCAB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZKYthlVrTos"
      },
      "source": [
        "You can also evaluate the tflite model with the `evaluate_tflite` method. This step is expected to take a long time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ochbq95ZrVFX"
      },
      "outputs": [],
      "source": [
        "model.evaluate_tflite('model.tflite', validation_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoWiA_zX8rxE"
      },
      "source": [
        "## Advanced Usage\n",
        "\n",
        "The `create` function is the critical part of this library in which the `model_spec` parameter defines the model specification. The `BertQASpec` class is currently supported. There are 2 models: MobileBERT model, BERT-Base model. The `create` function comprises the following steps:\n",
        "\n",
        "1. Creates the model for question answer according to `model_spec`.\n",
        "2. Train the question answer model.\n",
        "\n",
        "This section describes several advanced topics, including adjusting the model, tuning the training hyperparameters etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwtiksguDfhl"
      },
      "source": [
        "### Adjust the model\n",
        "\n",
        "You can adjust the model infrastructure like parameters `seq_len` and `query_len` in the `BertQASpec` class.\n",
        "\n",
        "Adjustable parameters for model:\n",
        "\n",
        "* `seq_len`: Length of the passage to feed into the model.\n",
        "* `query_len`: Length of the question to feed into the model.\n",
        "* `doc_stride`: The stride when doing a sliding window approach to take chunks of the documents.\n",
        "* `initializer_range`: The stdev of the truncated_normal_initializer for initializing all weight matrices.\n",
        "* `trainable`: Boolean, whether pre-trained layer is trainable.\n",
        "\n",
        "Adjustable parameters for training pipeline:\n",
        "\n",
        "* `model_dir`: The location of the model checkpoint files. If not set, temporary directory will be used.\n",
        "* `dropout_rate`: The rate for dropout.\n",
        "* `learning_rate`: The initial learning rate for Adam.\n",
        "* `predict_batch_size`: Batch size for prediction.\n",
        "* `tpu`: TPU address to connect to. Only used if using tpu.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAOd5_bzH9AQ"
      },
      "source": [
        "For example, you can train the model with a longer sequence length. If you change the model, you must first construct a new `model_spec`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9WBN0UTQoMN"
      },
      "outputs": [],
      "source": [
        "new_spec = model_spec.get('mobilebert_qa')\n",
        "new_spec.seq_len = 512"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LSTdghTP0Cv"
      },
      "source": [
        "The remaining steps are the same. Note that you must rerun both the `dataloader` and `create` parts as different model specs may have different preprocessing steps.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvQuy7RSDir3"
      },
      "source": [
        "### Tune training hyperparameters\n",
        "You can also tune the training hyperparameters like `epochs` and `batch_size` to impact the model performance. For instance,\n",
        "\n",
        "*   `epochs`: more epochs could achieve better performance, but may lead to overfitting.\n",
        "*   `batch_size`: number of samples to use in one training step.\n",
        "\n",
        "For example, you can train with more epochs and with a bigger batch size like:\n",
        "\n",
        "```python\n",
        "model = question_answer.create(train_data, model_spec=spec, epochs=5, batch_size=64)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eq6B9lKMfhS6"
      },
      "source": [
        "### Change the Model Architecture\n",
        "\n",
        "You can change the base model your data trains on by changing the `model_spec`. For example, to change to the BERT-Base model, run:\n",
        "\n",
        "```python\n",
        "spec = model_spec.get('bert_qa')\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2d7yycrgu6L"
      },
      "source": [
        "The remaining steps are the same."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFQrDMXzOVoB"
      },
      "source": [
        "### Customize Post-training quantization on the TensorFlow Lite model\n",
        "\n",
        "[Post-training quantization](https://www.tensorflow.org/lite/performance/post_training_quantization) is a conversion technique that can reduce model size and inference latency, while also improving CPU and hardware accelerator inference speed, with a little degradation in model accuracy. Thus, it's widely used to optimize the model.\n",
        "\n",
        "Model Maker library applies a default post-training quantization techique when exporting the model. If you want to customize post-training quantization, Model Maker supports multiple post-training quantization options using [QuantizationConfig](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/config/QuantizationConfig) as well. Let's take float16 quantization as an instance. First, define the quantization config.\n",
        "\n",
        "```python\n",
        "config = QuantizationConfig.for_float16()\n",
        "```\n",
        "\n",
        "\n",
        "Then we export the TensorFlow Lite model with such configuration.\n",
        "\n",
        "```python\n",
        "model.export(export_dir='.', tflite_filename='model_fp16.tflite', quantization_config=config)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPVopCeB6LV6"
      },
      "source": [
        "# Read more\n",
        "\n",
        "You can read our [BERT Question and Answer](https://www.tensorflow.org/lite/examples/bert_qa/overview) example to learn technical details. For more information, please refer to:\n",
        "\n",
        "*   TensorFlow Lite Model Maker [guide](https://www.tensorflow.org/lite/models/modify/model_maker) and [API reference](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker).\n",
        "*   Task Library: [BertQuestionAnswerer](https://www.tensorflow.org/lite/inference_with_metadata/task_library/bert_question_answerer) for deployment.\n",
        "*   The end-to-end reference apps: [Android](https://github.com/tensorflow/examples/tree/master/lite/examples/bert_qa/android) and [iOS](https://github.com/tensorflow/examples/tree/master/lite/examples/bert_qa/ios)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Model Maker Question Answer Tutorial",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}